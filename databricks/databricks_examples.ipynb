{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94721ec9",
   "metadata": {},
   "source": [
    "# Databricks Examples - Complete Beginner's Guide\n",
    "\n",
    "This notebook teaches you key Databricks & PySpark concepts through runnable examples. Each section has explanations designed for beginners.\n",
    "\n",
    "**What you'll learn:**\n",
    "- DataFrames: distributed tables that scale\n",
    "- Delta Lake: reliable, ACID-compliant storage\n",
    "- Database connections, partitioning, and schema evolution\n",
    "- Advanced features like MERGE and time-travel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b24929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session (Databricks already provides `spark`)\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(\"local-demo\").getOrCreate()\n",
    "print('Spark session ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) DataFrame transformation example\n",
    "df = spark.createDataFrame([(1, 'alice', 30), (2, 'bob', 17)], ['id','name','age'])\n",
    "from pyspark.sql.functions import col, when\n",
    "df2 = df.filter(col('age') > 18).withColumn('adult', when(col('age') >= 18, True).otherwise(False))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c5009",
   "metadata": {},
   "source": [
    "## 1. DataFrame Transformations\n",
    "\n",
    "### What is a DataFrame?\n",
    "A **DataFrame** is like an Excel spreadsheet or SQL table in Spark—it has rows and columns. You can filter rows, add/remove columns, and combine data.\n",
    "\n",
    "### Why use it?\n",
    "- Works across distributed machines (scales to petabytes of data)\n",
    "- SQL-like operations are optimized automatically by Spark's engine\n",
    "- Familiar syntax if you know SQL or pandas\n",
    "\n",
    "### How it works:\n",
    "1. **Create** a DataFrame from data (Python list, CSV, Parquet, etc.)\n",
    "2. **Transform** it: filter rows, add columns, group, join\n",
    "3. **Execute** an action to see results (e.g., `.show()`, `.count()`)\n",
    "\n",
    "**Think of it like a recipe:** transformations are instructions; `.show()` executes the recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6caa52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Delta write/read example (requires Delta runtime)\n",
    "path = '/tmp/delta_demo_table'\n",
    "try:\n",
    "    df2.write.format('delta').mode('overwrite').save(path)\n",
    "    df_read = spark.read.format('delta').load(path)\n",
    "    df_read.show()\n",
    "except Exception as e:\n",
    "    print('Delta write/read skipped (runtime may not support Delta):', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0f4516",
   "metadata": {},
   "source": [
    "## 2. Delta Lake Read/Write\n",
    "\n",
    "### What is Delta Lake?\n",
    "**Delta Lake** is a storage format built on top of object storage (S3/ADLS). It's like a \"super\" Parquet:\n",
    "- **ACID transactions**: Multiple writes don't corrupt data\n",
    "- **Schema enforcement**: Prevents bad data from entering your table\n",
    "- **Time travel**: Read data from a past version (e.g., before a mistake)\n",
    "- **Transaction log** (`_delta_log/`): Git-like commit history for your data\n",
    "\n",
    "### Why use it?\n",
    "- Reliability: No more corrupt data files\n",
    "- Easy auditing: See exactly what changed and when\n",
    "- Data quality: Enforce column types and constraints\n",
    "\n",
    "### How it works:\n",
    "When you write to Delta, Spark:\n",
    "1. Validates the schema (does it match the table?)\n",
    "2. Writes data as Parquet files\n",
    "3. Records a JSON entry in `_delta_log/` as proof of the transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5033f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) UDF example (normal UDF)\n",
    "from pyspark.sql.functions import udf, col\n",
    "import pyspark.sql.types as T\n",
    "@udf(T.IntegerType())\n",
    "def add_one(x):\n",
    "    return x + 1 if x is not None else None\n",
    "df.withColumn('age_plus_one', add_one(col('age'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8a490",
   "metadata": {},
   "source": [
    "## 3. User-Defined Functions (UDFs)\n",
    "\n",
    "### What is a UDF?\n",
    "A **UDF** (User-Defined Function) is your own custom function that runs on each row of data. Use it when built-in functions don't do what you need.\n",
    "\n",
    "### Example use cases:\n",
    "- Custom string cleaning (e.g., extract middle name from \"John Q. Smith\")\n",
    "- Domain-specific calculations (e.g., mortgage amortization)\n",
    "- Call external APIs (slower, but sometimes necessary)\n",
    "\n",
    "### ⚠️ Performance trade-off:\n",
    "UDFs are slower than built-in Spark functions because:\n",
    "- Spark can't optimize them (it doesn't know what they do)\n",
    "- Data crosses from Spark (JVM) ↔ Python (slow serialization)\n",
    "\n",
    "**Best practice**: Use built-in functions when possible. Only use UDFs when you really need custom logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) JDBC read example (placeholder credentials)\n",
    "jdbc_url = 'jdbc:postgresql://<host>:5432/<db>'\n",
    "jdbc_props = {\n",
    "    'user': '<username>',\n",
    "    'password': '<password>',\n",
    "    'driver': 'org.postgresql.Driver'\n",
    "}\n",
    "# Replace above with secrets (Databricks secrets or environment vars)\n",
    "try:\n",
    "    df_jdbc = spark.read.format('jdbc')\n",
    "        .option('url', jdbc_url)\n",
    "        .option('dbtable', 'public.my_table')\n",
    "        .option('user', jdbc_props['user'])\n",
    "        .option('password', jdbc_props['password'])\n",
    "        .load()\n",
    "    df_jdbc.show(5)\n",
    "except Exception as e:\n",
    "    print('JDBC example skipped (fill credentials or run in environment with driver):', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336eb86f",
   "metadata": {},
   "source": [
    "## 4. JDBC (Connecting to Databases)\n",
    "\n",
    "### What is JDBC?\n",
    "**JDBC** (Java Database Connectivity) is a standard way to connect Spark to databases like PostgreSQL, MySQL, Oracle, SQL Server, etc.\n",
    "\n",
    "### Why use it?\n",
    "- Read data from existing databases into Spark DataFrames\n",
    "- Write Spark results back to databases\n",
    "- Useful for ETL: extract from DB → transform in Spark → load back\n",
    "\n",
    "### Security note:\n",
    "**Never hardcode passwords!** Use:\n",
    "- Databricks Secrets (recommended)\n",
    "- Environment variables\n",
    "- AWS/Azure managed secrets\n",
    "\n",
    "### How it works:\n",
    "1. Spark opens multiple JDBC connections (one per partition)\n",
    "2. Each partition reads its data in parallel\n",
    "3. Results come back as a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ddd6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Metastore-managed Delta table: create and write\n",
    "table_name = 'default.demo_table'\n",
    "path = '/tmp/demo_table_path'\n",
    "# create table if not exists (uses Delta when supported)\n",
    "try:\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {table_name} (id LONG, name STRING, age INT) USING DELTA LOCATION '{path}'\")\n",
    "    # write DataFrame into table\n",
    "    df2.write.format('delta').mode('append').saveAsTable(table_name)\n",
    "    spark.sql(f\"SELECT * FROM {table_name} LIMIT 5\").show()\n",
    "except Exception as e:\n",
    "    print('Metastore table example skipped or failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f222e1",
   "metadata": {},
   "source": [
    "## 5. Metastore Tables (Registering Data)\n",
    "\n",
    "### What is the Metastore?\n",
    "The **metastore** is a catalog that keeps track of all your tables—like a library index.\n",
    "- Stores table names, schemas, and file locations\n",
    "- Allows you to query tables by name (e.g., `SELECT * FROM my_table`)\n",
    "- Multiple users can find and reuse tables\n",
    "\n",
    "### Managed vs. External tables?\n",
    "- **Managed** (default): Databricks manages both the data AND the location. Dropping the table deletes the data.\n",
    "- **External** (with LOCATION): You manage the data; Databricks just points to it. Dropping the table keeps the data.\n",
    "\n",
    "### Why use it?\n",
    "- **Discovery**: See all available data in one place\n",
    "- **Reusability**: Write once, query many times without reloading\n",
    "- **SQL interface**: Run SQL queries like traditional databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Partitioning and basic compaction pattern\n",
    "df_big = spark.createDataFrame([(1,'alice','us',30),(2,'bob','us',17),(3,'cara','eu',25)], ['id','name','region','age'])\n",
    "out_path = '/tmp/delta_partitioned'\n",
    "# write partitioned by region\n",
    "try:\n",
    "    df_big.write.format('delta').mode('overwrite').partitionBy('region').save(out_path)\n",
    "    # read back partitioned table\n",
    "    spark.read.format('delta').load(out_path).show()\n",
    "    # simple compaction: coalesce files before write (local workaround)\n",
    "    df_big.coalesce(1).write.format('delta').mode('overwrite').partitionBy('region').save(out_path + '_compacted')\n",
    "    print('Wrote partitioned and compacted (coalesce) datasets')\n",
    "except Exception as e:\n",
    "    print('Partitioning example skipped or failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff08b52",
   "metadata": {},
   "source": [
    "## 6. Partitioning & Compaction\n",
    "\n",
    "### What is Partitioning?\n",
    "**Partitioning** means organizing files by column values (e.g., by region, date, customer_id).\n",
    "\n",
    "**Without partitioning:**\n",
    "```\n",
    "/data/table/part-0001.parquet  (1GB, mixed regions)\n",
    "/data/table/part-0002.parquet  (1GB, mixed regions)\n",
    "```\n",
    "To read US data, Spark reads ALL files (2GB).\n",
    "\n",
    "**With partitioning by region:**\n",
    "```\n",
    "/data/table/region=us/part-0001.parquet (500MB)\n",
    "/data/table/region=eu/part-0002.parquet (500MB)\n",
    "```\n",
    "To read US data, Spark reads only the `region=us` folder (500MB). **50% faster!**\n",
    "\n",
    "### The Small-File Problem:\n",
    "Partitioning often creates many tiny files, slowing reads. **Compaction** combines them:\n",
    "- Use `.coalesce(1)` before write (forces 1 file per partition)\n",
    "- Use `OPTIMIZE` in Databricks (compacts small files automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ee184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Schema evolution: mergeSchema pattern (append new column)\n",
    "base = spark.createDataFrame([(1,'alice')], ['id','name'])\n",
    "path_schema = '/tmp/delta_schema_demo'\n",
    "try:\n",
    "    base.write.format('delta').mode('overwrite').save(path_schema)\n",
    "    # new data has an extra column 'age'\n",
    "    new_df = spark.createDataFrame([(2,'bob',28)], ['id','name','age'])\n",
    "    # unsafe append without mergeSchema will fail if enforcement is strict\n",
    "    new_df.write.format('delta').mode('append').option('mergeSchema','true').save(path_schema)\n",
    "    spark.read.format('delta').load(path_schema).show()\n",
    "except Exception as e:\n",
    "    print('Schema evolution example skipped or failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4887a2e",
   "metadata": {},
   "source": [
    "## 7. Schema Evolution\n",
    "\n",
    "### What is Schema Evolution?\n",
    "**Schema** = table structure (column names & types).\n",
    "**Schema evolution** = adding/removing columns over time as requirements change.\n",
    "\n",
    "### Example:\n",
    "- **v1 of data**: `id, name` (deployed 6 months ago)\n",
    "- **v2 of data**: `id, name, age` (new requirement this week)\n",
    "\n",
    "Question: Can we append v2 data to the same table?\n",
    "\n",
    "### The Challenge:\n",
    "Delta enforces schema by default—v2 data with an extra `age` column won't match v1 schema → **write fails**.\n",
    "\n",
    "### Solution: `mergeSchema=true`\n",
    "- Tells Delta: \"Accept this new column, add it to the schema\"\n",
    "- Old rows get `NULL` for the new `age` column\n",
    "- New rows fill in `age` values\n",
    "\n",
    "### ⚠️ Caution:\n",
    "`mergeSchema=true` is convenient but can silently change your schema. Better: plan schema changes explicitly with a migration script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e8b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) MERGE INTO and time travel (version AS OF) demo\n",
    "from pyspark.sql import Row\n",
    "merge_table = 'default.merge_demo'\n",
    "merge_path = '/tmp/merge_demo_path'\n",
    "try:\n",
    "    # initial data\n",
    "    init = spark.createDataFrame([Row(id=1, name='alice'), Row(id=2, name='bob')])\n",
    "    init.write.format('delta').mode('overwrite').option('overwriteSchema','true').save(merge_path)\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS {merge_table} USING DELTA LOCATION '{merge_path}'\")\n",
    "    # upserts\n",
    "    updates = spark.createDataFrame([Row(id=2, name='robert'), Row(id=3, name='cara')])\n",
    "    # perform MERGE via SQL (Databricks supports MERGE INTO).\n",
    "    # Register updates as temp view\n",
    "    updates.createOrReplaceTempView('updates_view')\n",
    "    spark.sql(f\"MERGE INTO {merge_table} t USING updates_view s ON t.id = s.id WHEN MATCHED THEN UPDATE SET * WHEN NOT MATCHED THEN INSERT *\")\n",
    "    print('After MERGE:')\n",
    "    spark.sql(f\"SELECT * FROM {merge_table} ORDER BY id\").show()\n",
    "    # time travel: read previous version (version 0)\n",
    "    try:\n",
    "        print('Time travel - version 0:')\n",
    "        spark.read.format('delta').option('versionAsOf', 0).load(merge_path).show()\n",
    "    except Exception as te:\n",
    "        print('Time-travel may not be supported in this runtime:', te)\n",
    "except Exception as e:\n",
    "    print('MERGE/time-travel example skipped or failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea2f716",
   "metadata": {},
   "source": [
    "## 8. MERGE INTO (Upsert) & Time Travel\n",
    "\n",
    "### What is MERGE INTO?\n",
    "**MERGE INTO** is an SQL command for upserts: \"Update if exists, insert if not.\" Like a smart merge.\n",
    "\n",
    "**Example:** You have a customer table and new updates come in:\n",
    "- Customer 2 (existing): Update their name from \"bob\" → \"robert\"\n",
    "- Customer 3 (new): Insert a new customer\n",
    "\n",
    "Without MERGE: complex logic (check if exists, then UPDATE or INSERT).\n",
    "With MERGE: one statement handles both.\n",
    "\n",
    "### What is Time Travel?\n",
    "**Time travel** lets you read data from a past version of your table.\n",
    "\n",
    "**Why useful?**\n",
    "- Recover from accidental deletes/overwrites\n",
    "- Audit changes: \"What did this table look like last week?\"\n",
    "- Debug: \"When did this bad data appear?\"\n",
    "\n",
    "**How:** Use `versionAsOf` (by transaction number) or `timestampAsOf` (by time).\n",
    "\n",
    "### Analogy:\n",
    "Think of Delta's transaction log like Git version control for your data. MERGE is a commit with multiple changes. Time travel is checking out an old commit to see history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2e323",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "You've learned:\n",
    "1. **DataFrames**: Distributed tables that scale across clusters\n",
    "2. **Delta Lake**: Reliable storage with ACID, schema enforcement, and time travel\n",
    "3. **UDFs**: Custom functions (use sparingly; prefer built-ins)\n",
    "4. **JDBC**: Connect to databases for ETL pipelines\n",
    "5. **Metastore**: Catalog of tables for discoverability\n",
    "6. **Partitioning**: Organize data for faster queries\n",
    "7. **Schema Evolution**: Add columns safely with `mergeSchema`\n",
    "8. **MERGE INTO**: Upsert logic in one SQL statement\n",
    "9. **Time Travel**: Read past versions for auditing/recovery\n",
    "\n",
    "### Next Steps:\n",
    "- Run this notebook in Databricks or local PySpark environment\n",
    "- Experiment: modify data, try different filters, add your own transformations\n",
    "- Read [Databricks Documentation](https://docs.databricks.com) for advanced topics\n",
    "- Try streaming (readStream/writeStream) for real-time data\n",
    "- Explore advanced optimizations (Z-ordering, OPTIMIZE, VACUUM)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
